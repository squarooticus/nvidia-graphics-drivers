From c948faa5b134de0748c9a186895ac38a52b026ff Mon Sep 17 00:00:00 2001
From: Andreas Beckmann <anbe@debian.org>
Date: Sun, 6 Aug 2023 17:18:58 +0200
Subject: [PATCH] backport get_user_pages changes from 525.53 (uvm part)

---
 nvidia-uvm/uvm8_ats_ibm.c           | 5 ++---
 nvidia-uvm/uvm8_populate_pageable.c | 4 ++--
 nvidia-uvm/uvm8_tools.c             | 2 +-
 nvidia-uvm/uvm8_va_space_mm.c       | 3 +--
 4 files changed, 6 insertions(+), 8 deletions(-)

diff --git a/nvidia-uvm/uvm8_ats_ibm.c b/nvidia-uvm/uvm8_ats_ibm.c
index 0cbf949..61a6e5e 100644
--- a/nvidia-uvm/uvm8_ats_ibm.c
+++ b/nvidia-uvm/uvm8_ats_ibm.c
@@ -452,8 +452,7 @@ NV_STATUS uvm_ats_ibm_service_fault(uvm_gpu_va_space_t *gpu_va_space,
 {
     uvm_va_space_t *va_space = gpu_va_space->va_space;
     struct mm_struct *mm = va_space->va_space_mm.mm;
-    int write = (access_type >= UVM_FAULT_ACCESS_TYPE_WRITE);
-    int force = 0;
+    unsigned int gup_flags = (access_type >= UVM_FAULT_ACCESS_TYPE_WRITE) ? FOLL_WRITE : 0;
     struct page *page;
     char *mapping;
     int ret;
@@ -464,7 +463,7 @@ NV_STATUS uvm_ats_ibm_service_fault(uvm_gpu_va_space_t *gpu_va_space,
     uvm_assert_mmap_lock_locked(mm);
 
     // TODO: Bug 2103669: Service more than a single fault at a time
-    ret = NV_GET_USER_PAGES_REMOTE(NULL, mm, (unsigned long)fault_addr, 1, write, force, &page, NULL);
+    ret = NV_GET_USER_PAGES_REMOTE(mm, (unsigned long)fault_addr, 1, gup_flags, &page, NULL, NULL);
     if (ret < 0)
         return errno_to_nv_status(ret);
 
diff --git a/nvidia-uvm/uvm8_populate_pageable.c b/nvidia-uvm/uvm8_populate_pageable.c
index 546456d..f913ab5 100644
--- a/nvidia-uvm/uvm8_populate_pageable.c
+++ b/nvidia-uvm/uvm8_populate_pageable.c
@@ -39,7 +39,7 @@ NV_STATUS uvm_populate_pageable_vma(struct vm_area_struct *vma,
     unsigned long vma_size;
     unsigned long vma_num_pages;
     unsigned long outer = start + length;
-    const bool is_writable = (vma->vm_flags) & VM_WRITE;
+    unsigned int gup_flags = ((vma->vm_flags) & VM_WRITE) ? FOLL_WRITE : 0;
     const bool min_prot_ok = (vma->vm_flags & min_prot) == min_prot;
     struct mm_struct *mm = vma->vm_mm;
     bool uvm_managed_vma;
@@ -69,7 +69,7 @@ NV_STATUS uvm_populate_pageable_vma(struct vm_area_struct *vma,
     if (uvm_managed_vma)
         uvm_record_unlock_mmap_lock_read(mm);
 
-    ret = NV_GET_USER_PAGES(start, vma_num_pages, is_writable, 0, NULL, NULL);
+    ret = NV_GET_USER_PAGES(start, vma_num_pages, gup_flags, NULL, NULL);
 
     if (uvm_managed_vma)
         uvm_record_lock_mmap_lock_read(mm);
diff --git a/nvidia-uvm/uvm8_tools.c b/nvidia-uvm/uvm8_tools.c
index d515524..d4c9eb9 100644
--- a/nvidia-uvm/uvm8_tools.c
+++ b/nvidia-uvm/uvm8_tools.c
@@ -262,7 +262,7 @@ static NV_STATUS map_user_pages(NvU64 user_va, NvU64 size, void **addr, struct p
     }
 
     nv_mmap_read_lock(current->mm);
-    ret = NV_GET_USER_PAGES(user_va, num_pages, 1, 0, *pages, vmas);
+    ret = NV_GET_USER_PAGES(user_va, num_pages, FOLL_WRITE, *pages, vmas);
     nv_mmap_read_unlock(current->mm);
     if (ret != num_pages) {
         status = NV_ERR_INVALID_ARGUMENT;
diff --git a/nvidia-uvm/uvm8_va_space_mm.c b/nvidia-uvm/uvm8_va_space_mm.c
index 2fdcc81..e98f63d 100644
--- a/nvidia-uvm/uvm8_va_space_mm.c
+++ b/nvidia-uvm/uvm8_va_space_mm.c
@@ -532,14 +532,13 @@ void uvm_va_space_mm_shutdown(uvm_va_space_t *va_space)
 static NV_STATUS mm_read64(struct mm_struct *mm, NvU64 addr, NvU64 *val)
 {
     long ret;
-    int write = 0, force = 0;
     struct page *page;
     NvU64 *mapping;
 
     UVM_ASSERT(IS_ALIGNED(addr, sizeof(val)));
 
     uvm_down_read_mmap_lock(mm);
-    ret = NV_GET_USER_PAGES_REMOTE(NULL, mm, (unsigned long)addr, 1, write, force, &page, NULL);
+    ret = NV_GET_USER_PAGES_REMOTE(mm, (unsigned long)addr, 1, 0, &page, NULL, NULL);
     uvm_up_read_mmap_lock(mm);
 
     if (ret < 0)
-- 
2.20.1

