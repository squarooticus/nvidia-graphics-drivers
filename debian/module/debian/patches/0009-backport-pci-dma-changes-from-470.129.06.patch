From 1712ab96128b776a65ab33f9798c3b9c5a1a0751 Mon Sep 17 00:00:00 2001
From: Andreas Beckmann <anbe@debian.org>
Date: Mon, 20 Jun 2022 10:14:54 +0200
Subject: [PATCH 1/2] backport pci/dma changes from 470.129.06

---
 nvidia/linux_nvswitch.c | 18 +++++++++---------
 nvidia/nv.c             | 18 +++++++++---------
 2 files changed, 18 insertions(+), 18 deletions(-)

diff --git a/nvidia/linux_nvswitch.c b/nvidia/linux_nvswitch.c
index 52ba24d..c4c390e 100644
--- a/nvidia/linux_nvswitch.c
+++ b/nvidia/linux_nvswitch.c
@@ -2058,11 +2058,11 @@ _nvswitch_to_pci_dma_direction
 )
 {
     if (direction == NVSWITCH_DMA_DIR_TO_SYSMEM)
-        return PCI_DMA_FROMDEVICE;
+        return DMA_FROM_DEVICE;
     else if (direction == NVSWITCH_DMA_DIR_FROM_SYSMEM)
-        return PCI_DMA_TODEVICE;
+        return DMA_TO_DEVICE;
     else
-        return PCI_DMA_BIDIRECTIONAL;
+        return DMA_BIDIRECTIONAL;
 }
 
 NvlStatus
@@ -2083,9 +2083,9 @@ nvswitch_os_map_dma_region
 
     dma_dir = _nvswitch_to_pci_dma_direction(direction);
 
-    *dma_handle = (NvU64)pci_map_single(pdev, cpu_addr, size, dma_dir);
+    *dma_handle = (NvU64)dma_map_single(&pdev->dev, cpu_addr, size, dma_dir);
 
-    if (pci_dma_mapping_error(pdev, *dma_handle))
+    if (dma_mapping_error(&pdev->dev, *dma_handle))
     {
         pr_err("nvidia-nvswitch: unable to create PCI DMA mapping\n");
         return -NVL_ERR_GENERIC;
@@ -2112,7 +2112,7 @@ nvswitch_os_unmap_dma_region
 
     dma_dir = _nvswitch_to_pci_dma_direction(direction);
 
-    pci_unmap_single(pdev, dma_handle, size, dma_dir);
+    dma_unmap_single(&pdev->dev, dma_handle, size, dma_dir);
 
     return NVL_SUCCESS;
 }
@@ -2129,7 +2129,7 @@ nvswitch_os_set_dma_mask
     if (!pdev)
         return -NVL_BAD_ARGS;
 
-    if (pci_set_dma_mask(pdev, DMA_BIT_MASK(dma_addr_width)))
+    if (dma_set_mask(&pdev->dev, DMA_BIT_MASK(dma_addr_width)))
         return -NVL_ERR_GENERIC;
 
     return NVL_SUCCESS;
@@ -2152,7 +2152,7 @@ nvswitch_os_sync_dma_region_for_cpu
 
     dma_dir = _nvswitch_to_pci_dma_direction(direction);
 
-    pci_dma_sync_single_for_cpu(pdev, dma_handle, size, dma_dir);
+    dma_sync_single_for_cpu(&pdev->dev, dma_handle, size, dma_dir);
 
     return NVL_SUCCESS;
 }
@@ -2174,7 +2174,7 @@ nvswitch_os_sync_dma_region_for_device
 
     dma_dir = _nvswitch_to_pci_dma_direction(direction);
 
-    pci_dma_sync_single_for_device(pdev, dma_handle, size, dma_dir);
+    dma_sync_single_for_device(&pdev->dev, dma_handle, size, dma_dir);
 
     return NVL_SUCCESS;
 }
diff --git a/nvidia/nv.c b/nvidia/nv.c
index d100006..ad9d74d 100644
--- a/nvidia/nv.c
+++ b/nvidia/nv.c
@@ -2805,7 +2805,7 @@ nv_set_dma_address_size(
      */
     if (!nvl->tce_bypass_enabled)
     {
-        pci_set_dma_mask(nvl->pci_dev, new_mask);
+        dma_set_mask(&nvl->pci_dev->dev, new_mask);
         /* Certain kernels have a bug which causes pci_set_consistent_dma_mask
          * to call GPL sme_active symbol, this bug has already been fixed in a
          * minor release update but detect the failure scenario here to prevent
@@ -4354,7 +4354,7 @@ NvU64 NV_API_CALL nv_get_dma_start_address(
      * Otherwise, the DMA start address only needs to be set once, and it
      * won't change afterward. Just return the cached value if asked again,
      * to avoid the kernel printing redundant messages to the kernel
-     * log when we call pci_set_dma_mask().
+     * log when we call dma_set_mask().
      */
     nvl = NV_GET_NVL_FROM_NV_STATE(nv);
     if ((nv_tce_bypass_mode == NV_TCE_BYPASS_MODE_DISABLE) ||
@@ -4405,19 +4405,19 @@ NvU64 NV_API_CALL nv_get_dma_start_address(
      * as the starting address for all DMA mappings.
      */
     saved_dma_mask = pci_dev->dma_mask;
-    if (pci_set_dma_mask(pci_dev, DMA_BIT_MASK(64)) != 0)
+    if (dma_set_mask(&pci_dev->dev, DMA_BIT_MASK(64)) != 0)
     {
         goto done;
     }
 
-    dma_addr = pci_map_single(pci_dev, NULL, 1, DMA_BIDIRECTIONAL);
-    if (pci_dma_mapping_error(pci_dev, dma_addr))
+    dma_addr = dma_map_single(&pci_dev->dev, NULL, 1, DMA_BIDIRECTIONAL);
+    if (dma_mapping_error(&pci_dev->dev, dma_addr))
     {
-        pci_set_dma_mask(pci_dev, saved_dma_mask);
+        dma_set_mask(&pci_dev->dev, saved_dma_mask);
         goto done;
     }
 
-    pci_unmap_single(pci_dev, dma_addr, 1, DMA_BIDIRECTIONAL);
+    dma_unmap_single(&pci_dev->dev, dma_addr, 1, DMA_BIDIRECTIONAL);
 
     /*
      * From IBM: "For IODA2, native DMA bypass or KVM TCE-based implementation
@@ -4449,7 +4449,7 @@ NvU64 NV_API_CALL nv_get_dma_start_address(
          */
         nv_printf(NV_DBG_WARNINGS,
             "NVRM: DMA window limited by platform\n");
-        pci_set_dma_mask(pci_dev, saved_dma_mask);
+        dma_set_mask(&pci_dev->dev, saved_dma_mask);
         goto done;
     }
     else if ((dma_addr & saved_dma_mask) != 0)
@@ -4468,7 +4468,7 @@ NvU64 NV_API_CALL nv_get_dma_start_address(
              */
             nv_printf(NV_DBG_WARNINGS,
                 "NVRM: DMA window limited by memory size\n");
-            pci_set_dma_mask(pci_dev, saved_dma_mask);
+            dma_set_mask(&pci_dev->dev, saved_dma_mask);
             goto done;
         }
     }
-- 
2.20.1

